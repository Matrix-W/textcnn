{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as ny\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "path = 'weibo_senti_100k.csv'\n",
    "outputs = 'weibo_senti_100k_random.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffles(path, outputs):\n",
    "    contents = []\n",
    "    f1 = open(path, 'r', encoding='utf-8') \n",
    "    for line in f1.readlines():\n",
    "        contents.append(line)\n",
    "    \n",
    "    random.shuffle(contents)\n",
    "    \n",
    "    f2 = open(outputs, 'w', encoding='utf-8')\n",
    "    for content in contents:\n",
    "        f2.write(content)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "\n",
    "\n",
    "shuffles(path,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119989, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(outputs, header=None, sep=',',encoding='utf-8')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "def cut(sentence):\n",
    "    return [token for token in jieba.lcut(sentence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "TEXT = torchtext.data.Field(sequential=True,lower=True,tokenize=cut)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(sequential=False, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/pt/17k8cfss7jj45568t8jjk4g40000gn/T/jieba.cache\n",
      "Loading model cost 0.852 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_dataset,dev_dataset,test_dataset = torchtext.data.TabularDataset.splits(\n",
    "      path='/Users/jimmy/Desktop/微博/sentiment-weibo/', \n",
    "      format='csv',   \n",
    "      skip_header=False,  \n",
    "      train='train.csv',  \n",
    "      validation='vali.csv',\n",
    "      test='test.csv',    \n",
    "      fields=[('label',LABEL),('content',TEXT)] \n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/365113 [00:00<?, ?it/s]Skipping token b'365113' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████▉| 364860/365113 [00:50<00:00, 11840.86it/s]"
     ]
    }
   ],
   "source": [
    "pretrained_name = 'sgns.sogounews.bigram-char' \n",
    "pretrained_path = '/Users/jimmy/Desktop/微博/sentiment-weibo/' \n",
    "vectors = torchtext.vocab.Vectors(name=pretrained_name, cache=pretrained_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, dev_dataset,test_dataset,\n",
    "                 vectors=vectors)\n",
    "LABEL.build_vocab(train_dataset, dev_dataset,test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197937"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter,test_iter = torchtext.data.BucketIterator.splits(\n",
    "        (train_dataset, dev_dataset,test_dataset), \n",
    "        batch_sizes=(128, 128,128), \n",
    "        sort_key=lambda x: len(x.content) \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 class_num, # 最后输出的种类数 \n",
    "                 filter_sizes, # 卷积核的长也就是滑动窗口的长 \n",
    "                 filter_num,   # 卷积核的数量 \n",
    "                 vocabulary_size, # 词表的大小\n",
    "                 embedding_dimension, # 词向量的维度\n",
    "                 vectors, # 词向量\n",
    "                 dropout): \n",
    "        super(TextCNN, self).__init__() \n",
    "\n",
    "        chanel_num = 1  # 通道数，也就是一篇文章一个样本只相当于一个feature map\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # 嵌入层 \n",
    "        self.embedding = self.embedding.from_pretrained(vectors) #嵌入层加载预训练词向量\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(chanel_num, filter_num, (fsz, embedding_dimension)) for fsz in filter_sizes])  # 卷积层\n",
    "        self.dropout = nn.Dropout(dropout) # dropout\n",
    "        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num) #全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x维度[句子长度,一个batch中所包含的样本数] 例:[3451,128]\n",
    "        x = self.embedding(x) # #经过嵌入层之后x的维度，[句子长度,一个batch中所包含的样本数,词向量维度] 例：[3451,128,300]\n",
    "        x = x.permute(1,0,2) # permute函数将样本数和句子长度换一下位置，[一个batch中所包含的样本数,句子长度,词向量维度] 例：[128,3451,300]\n",
    "        x = x.unsqueeze(1) # # conv2d需要输入的是一个四维数据，所以新增一维feature map数 unsqueeze(1)表示在第一维处新增一维，[一个batch中所包含的样本数,一个样本中的feature map数，句子长度,词向量维度] 例：[128,1,3451,300]\n",
    "        x = [conv(x) for conv in self.convs] # 与卷积核进行卷积，输出是[一个batch中所包含的样本数,卷积核数，句子长度-卷积核size+1,1]维数据,因为有[3,4,5]三张size类型的卷积核所以用列表表达式 例：[[128,16,3459,1],[128,16,3458,1],[128,16,3457,1]]\n",
    "        x = [sub_x.squeeze(3) for sub_x in x]#squeeze(3)判断第三维是否是1，如果是则压缩，如不是则保持原样 例：[[128,16,3459],[128,16,3458],[128,16,3457]]\n",
    "        x = [F.relu(sub_x) for sub_x in x] # ReLU激活函数激活，不改变x维度 \n",
    "        x = [F.max_pool1d(sub_x,sub_x.size(2)) for sub_x in x] # 池化层，根据之前说的原理，max_pool1d要取出每一个滑动窗口生成的矩阵的最大值，因此在第二维上取最大值 例：[[128,16,1],[128,16,1],[128,16,1]]\n",
    "        x = [sub_x.squeeze(2) for sub_x in x] # 判断第二维是否为1，若是则压缩 例：[[128,16],[128,16],[128,16]]\n",
    "        x = torch.cat(x, 1) # 进行拼接，例：[128,48]\n",
    "        x = self.dropout(x) # 去除掉一些神经元防止过拟合，注意dropout之后x的维度依旧是[128,48]，并不是说我dropout的概率是0.5，去除了一半的神经元维度就变成了[128,24]，而是把x中的一些神经元的数据根据概率全部变成了0，维度依旧是[128,48]\n",
    "        logits = self.fc(x) # 全接连层 例：输入x是[128,48] 输出logits是[128,10]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 2 # 类别数目\n",
    "filter_size = [3,4,5]  # 卷积核种类数 \n",
    "filter_num=16   # 卷积核数量\n",
    "vocab_size = len(TEXT.vocab) # 词表大小\n",
    "embedding_dim = TEXT.vocab.vectors.size()[-1] \n",
    "vectors = TEXT.vocab.vectors \n",
    "dropout=0.5 \n",
    "learning_rate = 0.001  \n",
    "epochs = 5   \n",
    "save_dir = './/TextCNN/model' # 模型保存路径\n",
    "steps_show = 10   # 每10步查看一次训练集loss和mini batch里的准确率\n",
    "steps_eval = 100  # 每100步测试一下验证集的准确率\n",
    "early_stopping = 1000  # 若发现当前验证集的准确率在1000步训练之后不再提高 一直小于best_acc,则提前停止训练\n",
    "\n",
    "textcnn_model = TextCNN(class_num=class_num,\n",
    "        filter_sizes=filter_size,\n",
    "        filter_num=filter_num,\n",
    "        vocabulary_size=vocab_size,\n",
    "        embedding_dimension=embedding_dim,\n",
    "        vectors=vectors,\n",
    "        dropout=dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, dev_iter, model):\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1): \n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.content, batch.label\n",
    "            if torch.cuda.is_available(): \n",
    "                feature,target = feature.cuda(),target.cuda() \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(feature)\n",
    "            loss = F.cross_entropy(logits, target) # 交叉熵损失函数\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            steps += 1 \n",
    "            if steps % steps_show == 0: \n",
    "                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum() # logits是[128,10],torch.max(logits, 1)也就是选出第一维中概率最大的值，输出为[128,1],torch.max(logits, 1)[1]相当于把每一个样本的预测输出取出来，然后通过view(target.size())平铺成和target一样的size (128,),然后把与target中相同的求和，统计预测正确的数量\n",
    "                train_acc = 100.0 * corrects / batch.batch_size # 计算每个mini batch中的准确率\n",
    "                print('steps:{} - loss: {:.6f}  acc:{:.4f}'.format(\n",
    "                  steps,\n",
    "                  loss.item(),\n",
    "                  train_acc))\n",
    "                \n",
    "            if steps % steps_eval == 0: # 每训练100步进行一次验证\n",
    "                dev_acc = dev_eval(dev_iter,model)\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    last_step = steps\n",
    "                    print('Saving best model, acc: {:.4f}%\\n'.format(best_acc))\n",
    "                    save(model,save_dir, steps)\n",
    "                else:\n",
    "                    if steps - last_step >= early_stopping:\n",
    "                        print('\\n提前停止于 {} steps, acc: {:.4f}%'.format(last_step, best_acc))\n",
    "                        raise KeyboardInterrupt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_eval(dev_iter,model):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in dev_iter:\n",
    "        feature, target = batch.content, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "        logits = model(feature)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                    [1].view(target.size()).data == target.data).sum()\n",
    "    size = len(dev_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                      accuracy,\n",
    "                                                                      corrects,\n",
    "                                                                      size))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, save_dir, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = 'bestmodel_steps{}.pt'.format(steps)\n",
    "    save_bestmodel_path = os.path.join(save_dir, save_path)\n",
    "    torch.save(model.state_dict(), save_bestmodel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:10 - loss: 0.486474  acc:78.9062\n",
      "steps:20 - loss: 0.328642  acc:89.0625\n",
      "steps:30 - loss: 0.201734  acc:94.5312\n",
      "steps:40 - loss: 0.093193  acc:97.6562\n",
      "steps:50 - loss: 0.069663  acc:98.4375\n",
      "steps:60 - loss: 0.137595  acc:96.0938\n",
      "steps:70 - loss: 0.105220  acc:96.8750\n",
      "steps:80 - loss: 0.042571  acc:99.2188\n",
      "steps:90 - loss: 0.059783  acc:96.8750\n",
      "steps:100 - loss: 0.076821  acc:99.2188\n",
      "\n",
      "Evaluation - loss: 0.000658  acc: 97.8998%(23494/23998) \n",
      "\n",
      "Saving best model, acc: 97.8998%\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-00b3259fc3e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtextcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-ee3dfa8c9306>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving best model, acc: {:.4f}%\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0d9ab8ddce27>\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, save_dir, steps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bestmodel_steps{}.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_bestmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "train(train_iter,dev_iter,textcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
